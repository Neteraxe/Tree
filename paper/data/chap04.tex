% !TeX root = ../Tree.tex

\chapter{机器学习中的树}

\section{决策树与ID3}

数据以以下形式记录（假设有n个特征）：
$$(X,Y)=(x_1,x_2,x_3,...,x_n,Y)$$
其中X是一个向量，因变量$Y$是一个标量，同时是我们试图理解、分类或概括的目标变量。向量$X$由用于该任务的属性$x_1,x_2,x_3...x_n$等组成。

假设所有输入特征都有有限的离散域，并且有一个称为“分类”的单一目标特征(target feature)。分类域$Y$的每个元素称为类。决策树或分类树是一种树，其中每个内部（非叶）节点都用输入特征（属性）标记。来自标记有输入特征（属性）的节点的弧被标记有目标特征（分类域）Y的每个可能值，或者弧导致不同输入特征上的从属决策节点。树的每个叶子都用一个类或类上的概率分布来标记，表示数据集已被树分类为一个特定的类，或一个特定的概率分布（如果决策树构造良好，则该概率分布会向某些类的子集倾斜）\cite{enwiki:1027552736}。用类标记是分类树，用类上的概率分布来标记是回归树。

而分裂基于一组基于分类特征的分裂规则。这个过程以递归的方式在每个派生子集上重复，称为递归分裂。当一个节点上的子集具有目标变量的所有相同值时，或者当拆分不再为预测增加值时，递归就完成了。这种自上而下的决策树归纳过程称为TDIDT，是贪婪算法的一个例子。不同分裂的规则使用不同算法，即使用不同的度量标准来衡量“最佳”。这些通常测量子集内目标变量的同质性。将这些度量应用于每个候选子集，并将所得值组合（平均、求和等）以提供分裂质量的度量。

其中一种算法为ID3（迭代二分法），是Ross Quinlan发明的一种算法，用于从数据集生成决策树。ID3采用以下指标（metrics）评估分裂点：

熵$H(S)$是集合$S$中不确定性量的度量（即熵表征集合$S$）:
$$H(S) = \sum_{y\in Y}{-p(y)\log _{2}p(y)}$$
其中，
\begin{enumerate}
	\item $S$正在为其计算熵的当前数据集,这在ID3算法的每一步都会发生变化，在对属性进行拆分的情况下会变为上一个集合的子集，在递归终止前下会变为父集合的“同级”递归分区。
	\item $Y$为S中的分类域，即类别的集合。
	\item $p(y)$为类y中元素数与集合S元素数的比例(proportion)。
\end{enumerate}

当$H(S)=0$时，集合$S$被完全归类，即元素属于同一类。在ID3中，为每个剩余属性计算熵。熵最小的属性用于在这个迭代中拆分集合$S$。

信息增益(Information Gain)$IG(A)$是在属性$A$上拆分集合$S$之前到之后熵差异的度量。即，在属性$A$上拆分集合$S$之后，$S$中的不确定性减少的度量。

$$IG(S,A)=H(S)-\sum_{t\in T}{p(t)H(t)}=H(S)-H(S|x)$$

其中，
\begin{enumerate}
	\item $H(S)$为集合$S$的熵。
	\item $T$为通过属性$x$拆分集合$S$而创建的子集，因此$S = \bigcup_{t\in T}^{t}$ 。
	\item $p(t)$为$t$中的元素数与集合$S$中元素数的比例。
	\item $H(t)$为子集$t$的熵。
\end{enumerate}
在ID3中，可以为每个剩余属性计算信息增益（而不是熵）。具有最大信息增益的属性用于在该迭代中拆分集合$S$。

ID3生成的树是一颗分类树，除了ID3还有CART（分类与回归树）,C4.5(ID3的继承者)等算法。

\section{树的集成学习}

集成学习通过构建多个决策树增加模型性能：

\begin{enumerate}
	\item Boost:通过训练每个新实例来强调以前错误建模的训练实例，从而逐步增强树的集成。一个典型的例子是AdaBoost。这些可用于回归类型和分类类型问题。通常用例有GBDT(梯度增强决策树)，XGBoost，LightGBM等。
	\item Bagging:Bagging（Bootstrap aggregated，Bootstrap采样聚合）决策树是一种早期的集成方法，它通过重复地对训练数据进行重采样和替换，并对树进行投票以获得一致性预测，从而构建多个决策树。随机森林是一种特定类型的Bootstrap采样聚合。
	\item 旋转森林-其中每个决策树都是通过首先对输入特征的随机子集应用主成分分析（PCA）来训练的。
\end{enumerate}

\section{随机森林}

森林是一组$n \ge 0$个不相交的树。

树转二叉树：
\begin{enumerate}
	\item 在所有兄弟结点之间加一连线。
	\item 对每个结点，除了保留与其第一个子结点的连线外，去掉该结点与其它孩子的连线。
	\item 水平调整。以树的根节点为轴，将整棵树顺时针旋转一定角度，使结构层次化(注意，第一个子节点是节点的左子节点，兄弟节点转换的子节点是节点的右子节点）。
\end{enumerate}

由于这样兄弟结点是右子树的一部分，孩子结点是左子树的一部分。从而成为二叉树。

森林转二叉树：
\begin{enumerate}
	\item 将每棵树转换为二叉树。
	\item 第一个二叉树不移动。从第二棵二叉树开始，后一棵二叉树的根节点作为前一棵二叉树的根节点的右子节点，并用线连接。
\end{enumerate}

随机森林或随机决策森林是一种集成学习方法，用于分类、回归和其他任务，通过在训练时构造多个决策树进行操作。对于分类任务，随机森林的输出是大多数树选择的类。对于回归任务，返回所有单个树的平均预测值。随机决策森林纠正了决策树过度拟合其训练集的习惯。随机森林通常优于决策树，但其精度低于梯度增强决策树。但是，数据特征会影响它们的性能。\cite{enwiki:1027325967}

第一个随机决策林算法是由Tin Kam Ho于1995年创建的使用随机子空间方法，在Ho的公式中，这是实现Eugene Kleinberg提出的“随机判别”分类方法的一种方法。随机森林经常被用作“黑箱”模型，因为它们可以在广泛的数据范围内生成合理的预测，而只需要很少的配置。



